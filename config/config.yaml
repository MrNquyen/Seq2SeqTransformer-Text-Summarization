
dataset_attributes:
  annotations:
    train: "train.json"
    val: "val.json"
    test: "test.json"
model_attributes:
  hidden_size: 768
  encoder:
    pretrained: /data2/npl/ViInfographicCaps/model/bert-base-uncased
    max_length: 312
    return_tensors: pt
  decoder:
    pretrained: /data2/npl/ViInfographicCaps/model/bert-base-uncased
    max_length: 128
    num_layers: 4
    nhead: 12
    max_length: 32
    dropout: 0.1
    return_tensors: pt
  adjust_optimizer:
    lr_scale: 0.1 # scale lr for finetuning modules
optimizer_attributes:
  lr_scale: 0.1 # scale lr for finetuning modules
  params:
    eps: 1.0e-08
    lr: 5e-5
    weight_decay: 0
  type: Adam
training_parameters:
  epochs: 26
  batch_size: 128
  iterations: 9000
  lr_scheduler:
    status: true
    type: "step"
    step_size: 7000
    gamma : 0.1
    lr_steps:
      - 6000
      - 10000
    lr_ratio: 0.1
    use_warmup: true
    warmup_factor: 0.2
    warmup_iterations: 1000
  max_iterations: 12000
  snapshot_interval: 300
  # snapshot_interval: 500
  monitored_metric: m4c_textcaps/textcaps_bleu4
  metric_minimize: false
  seed: 2021


    